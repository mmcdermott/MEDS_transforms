defaults:
  - pipeline
  - _self_

description: |-
  This pipeline extracts raw MEDS events in longitudinal, sparse form from an input dataset meeting select
  criteria and converts them to the flattened, MEDS format. It can be run in its entirety, with controllable
  levels of parallelism, or in stages. Arguments:
    - `event_conversion_config_fp`: The path to the event conversion configuration file. This file defines
      the events to extract from the various rows of the various input files encountered in the global input
      directory.
    - `input_dir`: The path to the directory containing the raw input files.
    - `cohort_dir`: The path to the directory where the output cohort will be written. It will be written in
      various subfolders of this dir depending on the stage, as intermediate stages cache their output during
      computation for efficiency of re-running and distributing.

# The event conversion configuration file is used throughout the pipeline to define the events to extract.
event_conversion_config_fp: ???
# The code modifier columns are in this pipeline only used in the aggregate_code_metadata stage.
code_modifiers: null
# The shards mapping is stored in the root of the final output directory.
shards_map_fp: "${cohort_dir}/splits.json"

stages:
  - shard_events
  - split_and_shard_patients
  - convert_to_sharded_events
  - merge_to_MEDS_cohort
  - aggregate_code_metadata

stage_configs:
  shard_events:
    description: |-
      This stage shards the raw input events into smaller files for easier processing. Arguments:
        - `row_chunksize`: The number of rows to read in at a time.
        - `infer_schema_length`: The number of rows to read in to infer the schema (only used if the source
          files are csvs)
    row_chunksize: 200000000
    infer_schema_length: 10000

  split_and_shard_patients:
    description: |-
      This stage splits the patients into training, tuning, and held-out sets, and further splits those sets
      into shards. Arguments:
        - `n_patients_per_shard`: The number of patients to include in a shard.
        - `external_splits_json_fp`: The path to a json file containing any pre-defined splits for specially
          held-out test sets beyond the IID held out set that will be produced (e.g., for prospective
          datasets, etc.).
        - `split_fracs`: The fraction of patients to include in the IID training, tuning, and held-out sets.
          Split fractions can be changed for the default names by adding a hydra-syntax command line argument
          for the nested name; e.g., `split_fracs.train=0.7 split_fracs.tuning=0.1 split_fracs.held_out=0.2`.
          A split can be removed with the `~` override Hydra syntax. Similarly, a new split name can be added
          with the standard Hydra `+` override option. E.g., `~split_fracs.held_out +split_fracs.test=0.1`. It
          is the user's responsibility to ensure that split fractions sum to 1.
    is_metadata: True
    output_dir: ${cohort_dir}
    n_patients_per_shard: 50000
    external_splits_json_fp: null
    split_fracs:
      train: 0.8
      tuning: 0.1
      held_out: 0.1

  merge_to_MEDS_cohort:
    description: |-
      This stage splits the patients into training, tuning, and held-out sets, and further splits those sets
      into shards. Arguments:
        - `n_patients_per_shard`: The number of patients to include in a shard.
        - `external_splits_json_fp`: The path to a json file containing any pre-defined splits for specially
          held-out test sets beyond the IID held out set that will be produced (e.g., for prospective
          datasets, etc.).
        - `split_fracs`: The fraction of patients to include in the IID training, tuning, and held-out sets.
    output_dir: ${cohort_dir}/final_cohort
    unique_by: "*"

  aggregate_code_metadata:
    description: |-
      This stage collects some descriptive metadata about the codes in the cohort. Arguments:
        - `aggregations`: The aggregations to compute over the codes. Defaults to counts of code occurrences,
          counts of patients with the code, and counts of value occurrences per code, as well as the sum and
          sum of squares of values (for use in computing means and variances).
    aggregations:
      - "code/n_occurrences"
      - "code/n_patients"
      - "values/n_occurrences"
      - "values/sum"
      - "values/sum_sqd"
    do_summarize_over_all_codes: true # This indicates we should include overall, code-independent counts
    is_metadata: True
    mapper_output_dir: "${cohort_dir}/code_metadata"
    output_dir: "${cohort_dir}"
    process_shard_prefix: "train/" # we only want to summarize over the training set
