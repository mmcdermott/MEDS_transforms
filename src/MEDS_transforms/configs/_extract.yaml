defaults:
  - _pipeline
  - stage_configs:
      - shard_events
      - split_and_shard_subjects
      - convert_to_sharded_events
      - merge_to_MEDS_cohort
      - extract_code_metadata
      - finalize_MEDS_metadata
      - finalize_MEDS_data
      # There is no configuration beyond the global "event_conversion_config_fp" for the
      # convert_to_sharded_events stage, so it doesn't have a stage config block here or below.
  - _self_

etl_metadata.pipeline_name: "extract"

description: |-
  This pipeline extracts raw MEDS events in longitudinal, sparse form from an input dataset meeting select
  criteria and converts them to the flattened, MEDS format. It can be run in its entirety, with controllable
  levels of parallelism, or in stages. Arguments:
    - `event_conversion_config_fp`: The path to the event conversion configuration file. This file defines
      the events to extract from the various rows of the various input files encountered in the global input
      directory.
    - `input_dir`: The path to the directory containing the raw input files.
    - `cohort_dir`: The path to the directory where the output cohort will be written. It will be written in
      various subfolders of this dir depending on the stage, as intermediate stages cache their output during
      computation for efficiency of re-running and distributing.

# The event conversion configuration file is used throughout the pipeline to define the events to extract.
event_conversion_config_fp: ???
# The shards mapping is stored in the root of the final output directory.
shards_map_fp: "${cohort_dir}/metadata/.shards.json"

stages:
  - shard_events
  - split_and_shard_subjects
  - convert_to_sharded_events
  - merge_to_MEDS_cohort
  - extract_code_metadata
  - finalize_MEDS_metadata
  - finalize_MEDS_data

stage_configs:
  shard_events:
    data_input_dir: "${input_dir}"
